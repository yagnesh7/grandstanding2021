{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio, clear_output, display\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = \"../outputs/data_transcripts_v2.csv\"\n",
    "transcripts = pd.read_csv(transcript_path)\n",
    "\n",
    "# # Only for sample purposes:\n",
    "# file_path = \"142-orig.wav\"\n",
    "# file_transcripts = transcripts.loc[transcripts[\"file\"] == file_path]\n",
    "\n",
    "bert_scores_path = \"../outputs/bert_scores_v2.csv\"\n",
    "bert_scores = pd.read_csv(bert_scores_path)\n",
    "\n",
    "transcripts = transcripts.merge(bert_scores, on=[\"file\", \"line\"])\n",
    "transcripts[\"line\"] = transcripts[\"line\"].astype(str)\n",
    "\n",
    "data_path = \"../outputs/npy2\"\n",
    "\n",
    "summary_data_path = \"../outputs/nn_summary_info.csv\"\n",
    "summary_data = pd.read_csv(summary_data_path)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "sequence_len = 2048\n",
    "n_features = 5\n",
    "\n",
    "write_dir = \"../outputs/splits/\"\n",
    "if not osp.exists(write_dir):\n",
    "    os.makedirs(write_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_csv(\"../outputs/valid_transcripts.csv\")\n",
    "transcripts = transcripts.merge(summary_data, on=[\"file\",\"line\"])\n",
    "print(transcripts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata: pd.DataFrame,\n",
    "        write_dir: str = \"./\",\n",
    "        data_dir: str = \"./\",\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 8,\n",
    "        y_name=\"gs_score\",\n",
    "        trunc_pad_len=2048,\n",
    "        in_dim: int = 5,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.write_dir = write_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.y_name = y_name\n",
    "        self.trunc_pad_len = trunc_pad_len\n",
    "        self.in_dim = in_dim\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Split out transcript metadata into train, val, test\n",
    "        rng = np.random.default_rng(42)\n",
    "        indices = rng.permutation(self.metadata.shape[0])\n",
    "        train_size = math.floor(len(indices) * 0.80)\n",
    "        val_size = math.floor(len(indices) * 0.10)\n",
    "        train_idx = indices[:train_size]\n",
    "        val_idx = indices[train_size : train_size + val_size]\n",
    "        test_idx = indices[train_size + val_size :]\n",
    "\n",
    "        self.train = self.metadata.iloc[train_idx].reset_index(drop=True)\n",
    "        self.train.to_csv(osp.join(self.write_dir, \"train.csv\"), index=False)\n",
    "\n",
    "        self.val = self.metadata.iloc[val_idx].reset_index(drop=True)\n",
    "        self.val.to_csv(osp.join(self.write_dir, \"val.csv\"), index=False)\n",
    "\n",
    "        self.test = self.metadata.iloc[test_idx].reset_index(drop=True)\n",
    "        self.test.to_csv(osp.join(self.write_dir, \"test.csv\"), index=False)\n",
    "\n",
    "    def setup(self):\n",
    "        # Load in train, val, test datasets\n",
    "        self.train_data = pd.read_csv(osp.join(self.write_dir, \"train.csv\"))\n",
    "        self.val_data = pd.read_csv(osp.join(self.write_dir, \"val.csv\"))\n",
    "        self.test_data = pd.read_csv(osp.join(self.write_dir, \"test.csv\"))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=dl.AudioDataset(\n",
    "                metadata=self.train_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dl.AudioDataset(\n",
    "                metadata=self.val_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dl.AudioDataset(\n",
    "                metadata=self.test_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 8\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_module = AudioDataModule(\n",
    "    metadata=transcripts,\n",
    "    write_dir=write_dir,\n",
    "    data_dir=data_path,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    y_name=\"gs_score\",\n",
    "    trunc_pad_len=sequence_len,\n",
    "    in_dim=n_features,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(data_module.train_dataloader()))\n",
    "print(one_batch[0].size())\n",
    "print(one_batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTMModel(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden=128, n_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, batch_first=True, num_layers=n_layers, dropout=0.2)\n",
    "        self.ffc = nn.Linear(n_hidden, n_hidden)\n",
    "        self.regressor = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1]\n",
    "        ff = self.ffc(out)\n",
    "        return self.regressor(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTMModelv2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTMPredictor(pl.LightningModule):\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        super().__init__()\n",
    "        self.model = AudioLSTMModel(n_features, n_hidden)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, outputs = self(x, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, outputs = self(x, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, outputs = self(x, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename = \"best-checkpoint\", \n",
    "    save_top_k=1, \n",
    "    verbose =True, \n",
    "    monitor = \"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"audio-model-v\")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience = 2)\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks = [early_stopping_callback],\n",
    "    max_epochs = N_EPOCHS,\n",
    "    gpus = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioLSTMPredictor(n_features = n_features, n_hidden=256)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ed0a0a4f111b3ce99ef225d8c8948128dfb8d887b4e95fbf3191dff77fb86c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
