{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f761da",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Audio, clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts/\")\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0abc4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca68d5",
   "metadata": {},
   "source": [
    "## Arguments & User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18510fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = \"../outputs/all_transcripts.csv\"\n",
    "transcripts = pd.read_csv(transcript_path)\n",
    "\n",
    "# # Only for sample purposes:\n",
    "# file_path = \"142-orig.wav\"\n",
    "# file_transcripts = transcripts.loc[transcripts[\"file\"] == file_path]\n",
    "\n",
    "bert_scores_path = \"../outputs/bert_scores.csv\"\n",
    "bert_scores = pd.read_csv(bert_scores_path)\n",
    "\n",
    "transcripts = transcripts.merge(bert_scores, on=[\"file\", \"line\"])\n",
    "transcripts[\"line\"] = transcripts[\"line\"].astype(str)\n",
    "\n",
    "data_path = \"../outputs/npy\"\n",
    "\n",
    "summary_data_path = \"../outputs/nn_summary_info.csv\"\n",
    "summary_data = pd.read_csv(summary_data_path)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "sequence_len = 2048\n",
    "n_features = 5\n",
    "\n",
    "write_dir = \"../outputs/splits/\"\n",
    "if not osp.exists(write_dir):\n",
    "    os.makedirs(write_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acdee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "npys = [path for path in Path(data_path).rglob(\"*.npy\")]\n",
    "\n",
    "npys_name = [n.name for n in npys]\n",
    "npys_name = pd.DataFrame(npys_name, columns=[\"name\"])\n",
    "sequences = npys_name.loc[~npys_name[\"name\"].str.contains(\"shape\")].copy()\n",
    "shapes = npys_name.loc[npys_name[\"name\"].str.contains(\"shape\")].copy()\n",
    "\n",
    "print(\"Sequence Arrays:\", len(sequences))\n",
    "sequences[\"splits\"] = sequences[\"name\"].apply(lambda x: x.split(\"_\"))\n",
    "sequences[\"file\"] = sequences[\"splits\"].apply(lambda x: x[0])\n",
    "sequences[\"line\"] = sequences[\"splits\"].apply(lambda x: x[1].split(\".\")[0])\n",
    "\n",
    "transcripts = transcripts.merge(\n",
    "    sequences[[\"file\", \"line\"]], on=[\"file\", \"line\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Matching Sequences:\", len(transcripts))\n",
    "try:\n",
    "  transcripts = transcripts.drop(\"Unnamed: 0\", axis = 1)\n",
    "except:\n",
    "  pass\n",
    "transcripts.sample(n=4)\n",
    "\n",
    "transcripts.to_csv(\"../outputs/valid_transcripts.csv\", index=False)\n",
    "\n",
    "transcripts = pd.read_csv(\"../outputs/valid_transcripts.csv\")\n",
    "transcripts = transcripts.merge(summary_data, on=[\"file\", \"line\"])\n",
    "print(transcripts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744b538",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "# Windows devices may have incompatability with the dataset defined in the notebook, please use the `../scripts/data_loader.py` import instead.\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata,\n",
    "        data_dir,\n",
    "        y_name=\"gs_score\",\n",
    "        trunc_pad_len=2048,\n",
    "        in_dim=35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Faster than using a .loc on column names directly\n",
    "        self.columns_dict = dict([(c, i) for i, c in enumerate(self.metadata.columns)])\n",
    "        self.data_dir = data_dir\n",
    "        self.y_name = y_name\n",
    "        self.trunc_pad_len = trunc_pad_len\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row with .iloc\n",
    "        row = self.metadata.iloc[idx]\n",
    "\n",
    "        means_stds = [\n",
    "            c for c in self.columns_dict.keys() if (\"mean\" in c) or (\"std\" in c)\n",
    "        ]\n",
    "\n",
    "        # self.in_dim += len(means_stds)\n",
    "\n",
    "        # Read in pre-computed numpy array\n",
    "        file_name = row[self.columns_dict[\"file\"]]\n",
    "        line_name = row[self.columns_dict[\"line\"]]\n",
    "        npy_path = osp.join(self.data_dir, f\"{file_name}_{line_name}.npy\")\n",
    "        data = np.load(npy_path)\n",
    "\n",
    "        # Get y_true\n",
    "        score = row[self.columns_dict[self.y_name]]\n",
    "\n",
    "        # Get averages and standard deviations of the features before padding.\n",
    "        summary_arr = row[[self.columns_dict[c] for c in means_stds]].values\n",
    "        summary_arr_tiled = np.tile(summary_arr, (data.shape[0], 1))\n",
    "\n",
    "        data = np.concatenate([data, summary_arr_tiled], axis=1)\n",
    "\n",
    "        # Pad/Truncate\n",
    "        data_aug = np.zeros((self.trunc_pad_len, self.in_dim))\n",
    "        data_aug[: min(data.shape[0], self.trunc_pad_len), :] = data[\n",
    "            : self.trunc_pad_len\n",
    "        ]\n",
    "        item = {\n",
    "            \"x\": torch.tensor(data_aug, dtype=torch.float),\n",
    "            \"y\": torch.tensor([score], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "        return (item[\"x\"], item[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1de0a2",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d20553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 metadata,\n",
    "                 data_maxes = np.load(\"../outputs/data_maxes.npy\"),\n",
    "                 split_directory=\"../outputs/splits\",\n",
    "                 data_directory=\"../outputs/npy\",\n",
    "                 seq_len=2048,\n",
    "                 num_features=5,\n",
    "                 y_col=\"gs_score\",\n",
    "                 batch_size=128,\n",
    "                 num_workers=4,\n",
    "                 seed=42\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.data_maxes = data_maxes\n",
    "        self.split_directory = split_directory\n",
    "        self.data_directory = data_directory\n",
    "        self.seq_len = seq_len\n",
    "        self.num_features = num_features\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        indices = rng.permutation(self.metadata.shape[0])\n",
    "        train_size = math.floor(len(indices) * 0.80)\n",
    "        val_size = math.floor(len(indices) * 0.10)\n",
    "        train_idx = indices[:train_size]\n",
    "        val_idx = indices[train_size : train_size + val_size]\n",
    "        test_idx = indices[train_size + val_size :]\n",
    "    \n",
    "        self.train = self.metadata.iloc[train_idx].reset_index(drop=True)\n",
    "        self.train.to_csv(osp.join(self.split_directory, \"train.csv\"), index=False)\n",
    "\n",
    "        self.val = self.metadata.iloc[val_idx].reset_index(drop=True)\n",
    "        self.val.to_csv(osp.join(self.split_directory, \"val.csv\"), index=False)\n",
    "\n",
    "        self.test = self.metadata.iloc[test_idx].reset_index(drop=True)\n",
    "        self.test.to_csv(osp.join(self.split_directory, \"test.csv\"), index=False)\n",
    "        \n",
    "    def setup(self):\n",
    "        self.train_data = pd.read_csv(osp.join(self.split_directory, \"train.csv\"))\n",
    "        self.val_data = pd.read_csv(osp.join(self.split_directory, \"val.csv\"))\n",
    "        self.test_data = pd.read_csv(osp.join(self.split_directory, \"test.csv\"))\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        self.train_dataset = dl.NewAudioDataset(metadata=self.train_data,\n",
    "            data_maxes = self.data_maxes,\n",
    "            data_directory=self.data_directory,\n",
    "            num_features= self.num_features,\n",
    "            seq_len = self.seq_len,\n",
    "            y_col=self.y_col)\n",
    "        train_loader = DataLoader(self.train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        self.val_dataset = dl.NewAudioDataset(metadata=self.val_data,\n",
    "            data_maxes = self.data_maxes,\n",
    "            data_directory=self.data_directory,\n",
    "            num_features= self.num_features,\n",
    "            seq_len = self.seq_len,\n",
    "            y_col=self.y_col)\n",
    "        val_loader = DataLoader(self.val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.test_dataset = dl.NewAudioDataset(\n",
    "            metadata=self.test_data,\n",
    "            data_maxes = self.data_maxes,\n",
    "            data_directory=self.data_directory,\n",
    "            num_features= self.num_features,\n",
    "            seq_len = self.seq_len,\n",
    "            y_col=self.y_col)\n",
    "        test_loader = DataLoader(self.test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebe6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = AudioDataModule(metadata=transcripts,\n",
    "    data_maxes = np.load(\"../outputs/data_maxes.npy\"),                    \n",
    "    split_directory=\"../outputs/splits\",\n",
    "    data_directory=\"../outputs/npy\",\n",
    "    seq_len=2048,\n",
    "    num_features=5,\n",
    "    y_col=\"gs_score\",\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4fe41b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2048, 5])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dm.train_dataloader()))\n",
    "print(one_batch[0].size())\n",
    "print(one_batch[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0acfc6",
   "metadata": {},
   "source": [
    "## Model Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80758874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    '''\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 hidden_size, \n",
    "                 seq_len, \n",
    "                 batch_size,\n",
    "                 num_layers, \n",
    "                 dropout, \n",
    "                 learning_rate,\n",
    "                 criterion):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            dropout=dropout, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07c56b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All parameters are aggregated in one place.\n",
    "This is useful for reporting experiment params to experiment tracking software\n",
    "\"\"\"\n",
    "\n",
    "p = dict(\n",
    "    seq_len=2048,\n",
    "    batch_size=128,\n",
    "    criterion=nn.MSELoss(),\n",
    "    max_epochs=50,\n",
    "    n_features=n_features,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0,\n",
    "    learning_rate=0.001,\n",
    "    bidirectional=False,\n",
    "    track_level_flag=False,\n",
    "    convnet_flag=False,\n",
    "    kernel_size=16,\n",
    "    stride=1,\n",
    "    conv_output=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63f6c445",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x0000024C8695ADF0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x0000024C8695ADF0>)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | criterion | MSELoss | 0     \n",
      "1 | lstm      | LSTM    | 201 K \n",
      "2 | linear    | Linear  | 129   \n",
      "--------------------------------------\n",
      "201 K     Trainable params\n",
      "0         Non-trainable params\n",
      "201 K     Total params\n",
      "0.805     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:406: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 5/5 [00:23<00:00,  4.79s/it, loss=0.56, v_num=0, train_loss=0.482, val_loss=0.468] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 100%|██████████| 1/1 [00:08<00:00,  8.59s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.5966285467147827}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 1/1 [00:08<00:00,  8.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5966285467147827}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename = \"best-checkpoint\", \n",
    "    save_top_k=1, \n",
    "    verbose =True, \n",
    "    monitor = \"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(\"lstm\", name=\"audio_change\")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience = 4)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=p['max_epochs'],\n",
    "    logger=logger,\n",
    "    gpus=1,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks = [early_stopping_callback],\n",
    "#     overfit_batches=1\n",
    ")\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    n_features = p['n_features'],\n",
    "    hidden_size = p['hidden_size'],\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size'],\n",
    "    criterion = p['criterion'],\n",
    "    num_layers = p['num_layers'],\n",
    "    dropout = p['dropout'],\n",
    "    learning_rate = p['learning_rate']\n",
    ")\n",
    "\n",
    "dm = AudioDataModule(metadata=transcripts,\n",
    "    data_maxes = np.load(\"../outputs/data_maxes.npy\"),                    \n",
    "    split_directory=\"../outputs/splits\",\n",
    "    data_directory=\"../outputs/npy\",\n",
    "    seq_len=2048,\n",
    "    num_features=5,\n",
    "    y_col=\"gs_score\",\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "dm.prepare_data()\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9eb394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
