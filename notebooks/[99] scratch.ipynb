{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-loads all imports every time the cell is ran. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('~/Downloads/household_power_consumption.txt/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerConsumptionDataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading \n",
    "      and processing work in one place.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''\n",
    "        Data is resampled to hourly intervals.\n",
    "        Both 'np.nan' and '?' are converted to 'np.nan'\n",
    "        'Date' and 'Time' columns are merged into 'dt' index\n",
    "        '''\n",
    "\n",
    "        if stage == 'fit' and self.X_train is not None:\n",
    "            return \n",
    "        if stage == 'test' and self.X_test is not None:\n",
    "            return\n",
    "        if stage is None and self.X_train is not None and self.X_test is not None:  \n",
    "            return\n",
    "        \n",
    "        path = '~/Downloads/household_power_consumption.txt/household_power_consumption.txt'\n",
    "        \n",
    "        df = pd.read_csv(\n",
    "            path, \n",
    "            sep=';', \n",
    "            parse_dates={'dt' : ['Date', 'Time']}, \n",
    "            infer_datetime_format=True, \n",
    "            low_memory=False, \n",
    "            na_values=['nan','?'], \n",
    "            index_col='dt'\n",
    "        )\n",
    "\n",
    "        df_resample = df.resample('h').mean()\n",
    "\n",
    "        X = df_resample.dropna().copy()\n",
    "        y = X['Global_active_power'].shift(-1).ffill()\n",
    "        self.columns = X.columns\n",
    "\n",
    "\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cv, y_cv, test_size=0.25, shuffle=False\n",
    "        )\n",
    "\n",
    "        preprocessing = StandardScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.X_test = preprocessing.transform(X_test)\n",
    "            self.y_test = y_test.values.reshape((-1, 1))\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, \n",
    "                                         self.y_test, \n",
    "                                         seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    '''\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 hidden_size, \n",
    "                 seq_len, \n",
    "                 batch_size,\n",
    "                 num_layers, \n",
    "                 dropout, \n",
    "                 learning_rate,\n",
    "                 criterion):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            dropout=dropout, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All parameters are aggregated in one place.\n",
    "This is useful for reporting experiment params to experiment tracking software\n",
    "'''\n",
    "\n",
    "p = dict(\n",
    "    seq_len = 24,\n",
    "    batch_size = 70, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 10,\n",
    "    n_features = 7,\n",
    "    hidden_size = 100,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=2)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\loggers\\csv_logs.py:57: UserWarning: Experiment logs directory ./lstm\\0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | criterion | MSELoss | 0     \n",
      "1 | lstm      | LSTM    | 43.6 K\n",
      "2 | linear    | Linear  | 101   \n",
      "--------------------------------------\n",
      "43.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.7 K    Total params\n",
      "0.175     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 1\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79776ab9752b456ea79ecf8a4f4ebf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c342f4125504508991f6920c13c39db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.24126406013965607}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.24126406013965607}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "csv_logger = CSVLogger('./', name='lstm', version='0'),\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=p['max_epochs'],\n",
    "    logger=csv_logger,\n",
    "    gpus=0,\n",
    "    progress_bar_refresh_rate=2,\n",
    ")\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    n_features = p['n_features'],\n",
    "    hidden_size = p['hidden_size'],\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size'],\n",
    "    criterion = p['criterion'],\n",
    "    num_layers = p['num_layers'],\n",
    "    dropout = p['dropout'],\n",
    "    learning_rate = p['learning_rate']\n",
    ")\n",
    "\n",
    "dm = PowerConsumptionDataModule(\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size']\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch x Sequence Length x Features\n",
    "next(iter(dm.train_dataloader()))[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6092a9a14935c0fda91fba9e22a173e78eada2dccd73d5f4bfbae38f5a7c8b38"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
