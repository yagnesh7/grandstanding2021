{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f761da",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c22d69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio, clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f2eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abc4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca68d5",
   "metadata": {},
   "source": [
    "## Arguments & User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18510fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = \"../outputs/data_transcripts_v2.csv\"\n",
    "transcripts = pd.read_csv(transcript_path)\n",
    "\n",
    "# # Only for sample purposes:\n",
    "# file_path = \"142-orig.wav\"\n",
    "# file_transcripts = transcripts.loc[transcripts[\"file\"] == file_path]\n",
    "\n",
    "bert_scores_path = \"../outputs/bert_scores_v2.csv\"\n",
    "bert_scores = pd.read_csv(bert_scores_path)\n",
    "\n",
    "transcripts = transcripts.merge(bert_scores, on=[\"file\", \"line\"])\n",
    "transcripts[\"line\"] = transcripts[\"line\"].astype(str)\n",
    "data_path = \"../outputs/npy2\"\n",
    "\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "sequence_len = 2048\n",
    "n_features = 5\n",
    "\n",
    "write_dir = \"../outputs/splits/\"\n",
    "if not osp.exists(write_dir):\n",
    "    os.makedirs(write_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3acdee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17110, 12)\n"
     ]
    }
   ],
   "source": [
    "# npys = [path for path in Path(data_path).rglob(\"*.npy\")]\n",
    "\n",
    "# npys_name = [n.name for n in npys]\n",
    "# npys_name = pd.DataFrame(npys_name, columns=[\"name\"])\n",
    "# sequences = npys_name.loc[~npys_name[\"name\"].str.contains(\"shape\")].copy()\n",
    "# shapes = npys_name.loc[npys_name[\"name\"].str.contains(\"shape\")].copy()\n",
    "\n",
    "# print(\"Sequence Arrays:\", len(sequences))\n",
    "# sequences[\"splits\"] = sequences[\"name\"].apply(lambda x: x.split(\"_\"))\n",
    "# sequences[\"file\"] = sequences[\"splits\"].apply(lambda x: x[0])\n",
    "# sequences[\"line\"] = sequences[\"splits\"].apply(lambda x: x[1].split(\".\")[0])\n",
    "\n",
    "# transcripts = transcripts.merge(\n",
    "#     sequences[[\"file\", \"line\"]], on=[\"file\", \"line\"]\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "# print(\"Matching Sequences:\", len(transcripts))\n",
    "# try:\n",
    "#   transcripts = transcripts.drop(\"Unnamed: 0\", axis = 1)\n",
    "# except:\n",
    "#   pass\n",
    "# transcripts.sample(n=4)\n",
    "\n",
    "# transcripts.to_csv(\"../outputs/valid_transcripts.csv\", index=False)\n",
    "transcripts = pd.read_csv(\"../outputs/valid_transcripts.csv\")\n",
    "print(transcripts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744b538",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e97c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, metadata, data_dir, y_name=\"gs_score\", trunc_pad_len=2048, in_dim=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Faster than using a .loc on column names directly\n",
    "        self.columns_dict = dict([(c, i) for i, c in enumerate(self.metadata.columns)])\n",
    "        self.data_dir = data_dir\n",
    "        self.y_name = y_name\n",
    "        self.trunc_pad_len = trunc_pad_len\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row with .iloc\n",
    "        row = self.metadata.iloc[idx]\n",
    "\n",
    "        # Read in pre-computed numpy array\n",
    "        file_name = row[self.columns_dict[\"file\"]]\n",
    "        line_name = row[self.columns_dict[\"line\"]]\n",
    "        npy_path = osp.join(self.data_dir, f\"{file_name}_{line_name}.npy\")\n",
    "        data = np.load(npy_path)\n",
    "\n",
    "        # Get y_true\n",
    "        score = row[self.columns_dict[self.y_name]]\n",
    "\n",
    "        # Pad/Truncate\n",
    "        data_aug = np.zeros((self.trunc_pad_len, self.in_dim))\n",
    "        data_aug[: min(data.shape[0], self.trunc_pad_len), :] = data[\n",
    "            : self.trunc_pad_len\n",
    "        ]\n",
    "        item = {\n",
    "            \"x\": torch.tensor(data_aug, dtype=torch.float),\n",
    "            \"y\": torch.tensor([score], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "        return (item[\"x\"], item[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c83e956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0100, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0100, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0100, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([-1.1736]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = AudioDataset(transcripts, data_path)\n",
    "next(iter(new_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1de0a2",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d20553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata: pd.DataFrame,\n",
    "        write_dir: str = \"./\",\n",
    "        data_dir: str = \"./\",\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 8,\n",
    "        y_name=\"gs_score\",\n",
    "        trunc_pad_len=2048,\n",
    "        in_dim: int = 5,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.write_dir = write_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.y_name = y_name\n",
    "        self.trunc_pad_len = trunc_pad_len\n",
    "        self.in_dim = in_dim\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Split out transcript metadata into train, val, test\n",
    "        rng = np.random.default_rng(42)\n",
    "        indices = rng.permutation(self.metadata.shape[0])\n",
    "        train_size = math.floor(len(indices) * 0.80)\n",
    "        val_size = math.floor(len(indices) * 0.10)\n",
    "        train_idx = indices[:train_size]\n",
    "        val_idx = indices[train_size : train_size + val_size]\n",
    "        test_idx = indices[train_size + val_size :]\n",
    "\n",
    "        self.train = self.metadata.iloc[train_idx].reset_index(drop=True)\n",
    "        self.train.to_csv(osp.join(self.write_dir, \"train.csv\"), index=False)\n",
    "\n",
    "        self.val = self.metadata.iloc[val_idx].reset_index(drop=True)\n",
    "        self.val.to_csv(osp.join(self.write_dir, \"val.csv\"), index=False)\n",
    "\n",
    "        self.test = self.metadata.iloc[test_idx].reset_index(drop=True)\n",
    "        self.test.to_csv(osp.join(self.write_dir, \"test.csv\"), index=False)\n",
    "\n",
    "    def setup(self):\n",
    "        # Load in train, val, test datasets\n",
    "        self.train_data = pd.read_csv(osp.join(self.write_dir, \"train.csv\"))\n",
    "        self.val_data = pd.read_csv(osp.join(self.write_dir, \"val.csv\"))\n",
    "        self.test_data = pd.read_csv(osp.join(self.write_dir, \"test.csv\"))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=dl.AudioDataset(\n",
    "                metadata=self.train_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dl.AudioDataset(\n",
    "                metadata=self.val_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dl.AudioDataset(\n",
    "                metadata=self.test_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ebe6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = AudioDataModule(\n",
    "    metadata=transcripts,\n",
    "    write_dir=write_dir,\n",
    "    data_dir=data_path,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    y_name=\"gs_score\",\n",
    "    trunc_pad_len=sequence_len,\n",
    "    in_dim=n_features,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "datamodule.prepare_data()\n",
    "\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4fe41b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2048, 5])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "one_batch = next(iter(datamodule.train_dataloader()))\n",
    "print(one_batch[0].size())\n",
    "print(one_batch[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0acfc6",
   "metadata": {},
   "source": [
    "## Model Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31761870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFFModel(pl.LightningModule):\n",
    "    def __init__(self, criterion, input_dims=(2048, 5), learning_rate = 0.001):\n",
    "        super(AudioFFModel,self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dims[1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l1 = nn.Linear(input_dims[0] * input_dims[1], int(input_dims[0] * input_dims[1]/4))\n",
    "        self.l2 = nn.Linear(int(input_dims[0] * input_dims[1]/4), int(input_dims[0] * input_dims[1]/16))\n",
    "        self.l3 = nn.Linear(int(input_dims[0] * input_dims[1]/16), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, *_):\n",
    "        x, y = batch[0], batch[1]\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *_):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, *_):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "      \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082b3cb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\yagne\\anaconda3\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\yagne\\anaconda3\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:118: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | batch_norm | BatchNorm1d | 10    \n",
      "1 | flatten    | Flatten     | 0     \n",
      "2 | relu       | ReLU        | 0     \n",
      "3 | l1         | Linear      | 26.2 M\n",
      "4 | l2         | Linear      | 1.6 M \n",
      "5 | l3         | Linear      | 641   \n",
      "-------------------------------------------\n",
      "27.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.9 M    Total params\n",
      "111.427   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/107 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yagne\\anaconda3\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000207EAFB5430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yagne\\anaconda3\\envs\\grandstanding\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\yagne\\anaconda3\\envs\\grandstanding\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1295, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()\n",
    "model = AudioFFModel(criterion=F.mse_loss)\n",
    "trainer.fit(model, datamodule.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80758874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        hidden_size,\n",
    "        seq_len,\n",
    "        batch_size,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        learning_rate,\n",
    "        criterion,\n",
    "        bidirectional,\n",
    "        track_level_flag,\n",
    "        convnet_flag,\n",
    "        kernel_size=None,\n",
    "        stride=None,\n",
    "        conv_output=None\n",
    "    ):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm_input = self.n_features\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(n_features)\n",
    "\n",
    "        self.track_level_flag = track_level_flag\n",
    "        if self.track_level_flag:\n",
    "            self.lstm_input = self.n_features*3 # original + mean + std\n",
    "        \n",
    "        self.convnet_flag = convnet_flag\n",
    "        if self.convnet_flag:\n",
    "            self.conv_input = self.lstm_input\n",
    "            self.kernel_size = kernel_size\n",
    "            self.stride = stride\n",
    "            self.conv_output = conv_output\n",
    "            self.conv1d = nn.Conv1d(self.conv_input, self.conv_output, self.kernel_size, self.stride, \"same\")\n",
    "            self.lstm_input = conv_output\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x_orig = (batch_size, seq_len, hidden_size)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        if self.track_level_flag:\n",
    "            x_means = x.mean(axis=1).unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "            x_stds =  x.std(axis=1).unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "            x = torch.cat((x, x_means, x_stds), dim = 2)\n",
    "\n",
    "        if self.convnet_flag:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.conv1d(x)\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:, -1])\n",
    "        return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, *_):\n",
    "        x, y = batch[0], batch[1]\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *_):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, *_):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c56b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All parameters are aggregated in one place.\n",
    "This is useful for reporting experiment params to experiment tracking software\n",
    "\"\"\"\n",
    "\n",
    "p = dict(\n",
    "    seq_len=2048,\n",
    "    batch_size=128,\n",
    "    criterion=nn.MSELoss(),\n",
    "    max_epochs=50,\n",
    "    n_features=5,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    learning_rate=0.001,\n",
    "    bidirectional=False,\n",
    "    track_level_flag = False,\n",
    "    convnet_flag=False,\n",
    "    kernel_size=16,\n",
    "    stride=1,\n",
    "    conv_output=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f6c445",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = LSTMRegressor(\n",
    "    n_features=p[\"n_features\"],\n",
    "    hidden_size=p[\"hidden_size\"],\n",
    "    seq_len=p[\"seq_len\"],\n",
    "    batch_size=p[\"batch_size\"],\n",
    "    criterion=p[\"criterion\"],\n",
    "    num_layers=p[\"num_layers\"],\n",
    "    dropout=p[\"dropout\"],\n",
    "    learning_rate=p[\"learning_rate\"],\n",
    "    bidirectional=p[\"bidirectional\"],\n",
    "    track_level_flag=p[\"track_level_flag\"],\n",
    "    convnet_flag=p[\"convnet_flag\"],\n",
    "    kernel_size=p[\"kernel_size\"],\n",
    "    stride=p[\"stride\"],\n",
    "    conv_output=p[\"conv_output\"],\n",
    ")\n",
    "\n",
    "dm = AudioDataModule(\n",
    "    metadata=transcripts,\n",
    "    write_dir=write_dir,\n",
    "    data_dir=data_path,\n",
    "    batch_size=64,\n",
    "    num_workers=num_workers,\n",
    "    y_name=\"gs_score\",\n",
    "    trunc_pad_len=sequence_len,\n",
    "    in_dim=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9eb394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | criterion | MSELoss     | 0     \n",
      "1 | bn        | BatchNorm1d | 10    \n",
      "2 | lstm      | LSTM        | 3.2 M \n",
      "3 | linear    | Linear      | 513   \n",
      "------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.659    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  86%|████████▌ | 207/241 [02:12<00:21,  1.56it/s, loss=0.452, v_num=14]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  41%|████      | 11/27 [00:18<00:04,  3.64it/s]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1,max_epochs=50, default_root_dir=\"./logs/\")\n",
    "\n",
    "trainer.fit(lstm_model, dm)\n",
    "trainer.test(lstm_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baa61261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | criterion | MSELoss     | 0     \n",
      "1 | bn        | BatchNorm1d | 10    \n",
      "2 | lstm      | LSTM        | 206 K \n",
      "3 | linear    | Linear      | 129   \n",
      "------------------------------------------\n",
      "206 K     Trainable params\n",
      "0         Non-trainable params\n",
      "206 K     Total params\n",
      "0.826     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 121/121 [00:55<00:00,  2.18it/s, loss=0.372, v_num=4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  93%|█████████▎| 13/14 [00:16<00:00,  2.68it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.394527792930603}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.394527792930603}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=50, default_root_dir=\"./logs/\",)\n",
    "\n",
    "trainer.fit(lstm_model, dm)\n",
    "trainer.test(lstm_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec95073",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = one_batch[0]\n",
    "x_means = x.mean(axis=1).unsqueeze(1).repeat(1, 2048, 1)\n",
    "# x_stds =  x.std(axis=1).unsqueeze(1).repeat(1, 2048, 1)\n",
    "# x = torch.cat((x, x_means, x_stds), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f5e9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4251764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7550, 0.3506, 0.1233, 0.4727, 0.1162])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7fbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
