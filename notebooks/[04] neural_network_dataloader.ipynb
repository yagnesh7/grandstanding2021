{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f761da",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c22d69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio, clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f2eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abc4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca68d5",
   "metadata": {},
   "source": [
    "## Arguments & User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18510fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = \"../outputs/all_transcripts.csv\"\n",
    "transcripts = pd.read_csv(transcript_path)\n",
    "\n",
    "# Only for sample purposes:\n",
    "file_path = \"142-orig.wav\"\n",
    "file_transcripts = transcripts.loc[transcripts[\"file\"] == file_path]\n",
    "\n",
    "bert_scores_path = \"../outputs/bert_scores.csv\"\n",
    "bert_scores = pd.read_csv(bert_scores_path)\n",
    "\n",
    "file_transcripts = file_transcripts.merge(bert_scores, on=[\"file\", \"line\"])\n",
    "\n",
    "data_path = \"../outputs/npy/\"\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "\n",
    "max_len = np.max(np.load(\"../outputs/npy/142-orig.wav_shapes.npy\"))\n",
    "write_dir = \"../outputs/splits/\"\n",
    "if not osp.exists(write_dir):\n",
    "    os.makedirs(write_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744b538",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e97c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "# NOTE: REIMPORTED DUE TO EMPTY ERROR IN `../scripts/data_loader.py`\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, metadata, data_dir, y_name=\"gs_score\", trunc_pad_len=2048, in_dim=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Faster than using a .loc on column names directly\n",
    "        self.columns_dict = dict([(c, i) for i, c in enumerate(self.metadata.columns)])\n",
    "        self.data_dir = data_dir\n",
    "        self.y_name = y_name\n",
    "        self.trunc_pad_len = trunc_pad_len\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row with .iloc\n",
    "        row = self.metadata.iloc[idx]\n",
    "\n",
    "        # Read in pre-computed numpy array\n",
    "        file_name = row[self.columns_dict[\"file\"]]\n",
    "        line_name = row[self.columns_dict[\"line\"]]\n",
    "        npy_path = osp.join(self.data_dir, f\"{file_name}_{line_name}.npy\")\n",
    "        data = np.load(npy_path)\n",
    "\n",
    "        # Get y_true\n",
    "        score = row[self.columns_dict[self.y_name]]\n",
    "\n",
    "        # Pad/Truncate\n",
    "        data_aug = np.zeros((self.trunc_pad_len, self.in_dim))\n",
    "        data_aug[: min(data.shape[0], self.trunc_pad_len), :] = data[\n",
    "            : self.trunc_pad_len\n",
    "        ]\n",
    "        item = {\n",
    "            \"x\": torch.tensor(data_aug, dtype=torch.float),\n",
    "            \"y\": torch.tensor([score], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c83e956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[0.0000, 0.0000, 0.0407, 0.0000, 0.0000],\n",
       "         [8.2564, 1.0000, 0.1086, 0.0000, 0.0000],\n",
       "         [8.2897, 1.0000, 0.0780, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " 'y': tensor([-0.5621])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = AudioDataset(file_transcripts, data_path)\n",
    "next(iter(new_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1de0a2",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d20553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata: pd.DataFrame,\n",
    "        write_dir: str = \"./\",\n",
    "        data_dir: str = \"./\",\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 8,\n",
    "        y_name=\"gs_score\",\n",
    "        trunc_pad_len=2048,\n",
    "        in_dim: int = 5,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.write_dir = write_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.y_name = y_name\n",
    "        self.trunc_pad_len = trunc_pad_len\n",
    "        self.in_dim = in_dim\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Split out transcript metadata into train, val, test\n",
    "        rng = np.random.default_rng(42)\n",
    "        indices = rng.permutation(self.metadata.shape[0])\n",
    "        train_size = math.floor(len(indices) * 0.80)\n",
    "        val_size = math.floor(len(indices) * 0.10)\n",
    "        train_idx = indices[:train_size]\n",
    "        val_idx = indices[train_size : train_size + val_size]\n",
    "        test_idx = indices[train_size + val_size :]\n",
    "\n",
    "        self.train = self.metadata.iloc[train_idx].reset_index(drop=True)\n",
    "        self.train.to_csv(osp.join(self.write_dir, \"train.csv\"), index=False)\n",
    "\n",
    "        self.val = self.metadata.iloc[val_idx].reset_index(drop=True)\n",
    "        self.val.to_csv(osp.join(self.write_dir, \"val.csv\"), index=False)\n",
    "\n",
    "        self.test = self.metadata.iloc[test_idx].reset_index(drop=True)\n",
    "        self.test.to_csv(osp.join(self.write_dir, \"test.csv\"), index=False)\n",
    "\n",
    "    def setup(self):\n",
    "        # Load in train, val, test datasets\n",
    "        self.train_data = pd.read_csv(osp.join(self.write_dir, \"train.csv\"))\n",
    "        self.val_data = pd.read_csv(osp.join(self.write_dir, \"val.csv\"))\n",
    "        self.test = pd.read_csv(osp.join(self.write_dir, \"test.csv\"))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=dl.AudioDataset(\n",
    "                metadata=self.train_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dl.AudioDataset(\n",
    "                metadata=self.val_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dl.AudioDataset(\n",
    "                metadata=self.test_data,\n",
    "                data_dir=self.data_dir,\n",
    "                y_name=self.y_name,\n",
    "                trunc_pad_len=self.trunc_pad_len,\n",
    "                in_dim=self.in_dim,\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = AudioDataModule(\n",
    "    metadata=file_transcripts,\n",
    "    write_dir=write_dir,\n",
    "    data_dir=data_path,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    y_name=\"gs_score\",\n",
    "    trunc_pad_len=2048,\n",
    "    in_dim=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "datamodule.prepare_data()\n",
    "\n",
    "datamodule.setup()\n",
    "\n",
    "next(iter(datamodule.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0acfc6",
   "metadata": {},
   "source": [
    "## Model Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31761870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFFModel(pl.LightningModule):\n",
    "    def __init__(self, input_dims=(2048, 5)):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dims[0])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(input_dims[0] * input_dims[1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"x\"], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "082b3cb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | batch_norm | BatchNorm1d | 4.1 K \n",
      "1 | flatten    | Flatten     | 0     \n",
      "2 | l1         | Linear      | 10.2 K\n",
      "3 | relu       | ReLU        | 0     \n",
      "-------------------------------------------\n",
      "14.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93ad1265eaa4fe7a79d24a290583ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yagne\\AppData\\Local\\Temp/ipykernel_21756/2521295752.py:18: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y)\n",
      "C:\\Users\\yagne\\.conda\\envs\\grandstanding\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()\n",
    "model = AudioFFModel()\n",
    "trainer.fit(model, datamodule.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa9d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
